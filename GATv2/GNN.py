import os
import glob
import random
import warnings

import torch
import torch.nn.functional as F
import torch.optim as optim
import numpy as np

from torch_geometric.loader import DataLoader

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt  # â˜… æ–°å¢ï¼šç”¨äºç”»æ•£ç‚¹å›¾

warnings.filterwarnings('ignore')

# ===== å¯¼å…¥ GNN æ¨¡å‹ =====
try:
    from gnn_model import GridGNN
except ImportError as e:
    print(f"âŒ ç¼ºå°‘ gnn_model.py æˆ– GridGNN å®šä¹‰: {e}")
    exit()

# ================= é…ç½®å‚æ•° =================
DATA_DIR   = "./dataset_output_1mv_urban"   # â˜… è¿™é‡Œè¦å’Œç”Ÿæˆæ•°æ®è„šæœ¬çš„ä¸€è‡´
LR         = 5e-4
EPOCHS     = 400
BATCH_SIZE = 32
HIDDEN_DIM = 128
HEADS      = 4
DEVICE     = torch.device("cuda" if torch.cuda.is_available() else "cpu")
SAVE_PATH  = "best_gnn_model_offline.pth"
SEED       = 0
# ===========================================

# å›ºå®šéšæœºç§å­ï¼Œä¿è¯å¯å¤ç°
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)


# ============ æ•°æ®åŠ è½½éƒ¨åˆ† ============

def load_all_chunks(data_dir):
    """
    ä»æŒ‡å®šç›®å½•è¯»å–æ‰€æœ‰ chunk_*.pt æ–‡ä»¶ï¼Œåˆå¹¶ä¸ºä¸€ä¸ª Data åˆ—è¡¨ã€‚
    æ¯ä¸ª chunk æ–‡ä»¶æ˜¯ä¸€ä¸ª [Data, Data, ...] çš„åˆ—è¡¨ã€‚
    """
    if not os.path.exists(data_dir):
        print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        exit()

    pattern = os.path.join(data_dir, "chunk_*.pt")
    files = sorted(glob.glob(pattern))
    if len(files) == 0:
        print(f"âŒ åœ¨ç›®å½• {data_dir} ä¸‹æ²¡æœ‰æ‰¾åˆ° chunk_*.pt æ–‡ä»¶")
        exit()

    all_data = []
    for f in files:
        try:
            chunk = torch.load(f, weights_only=False)

            # chunk åº”è¯¥æ˜¯ä¸€ä¸ª Data åˆ—è¡¨
            if isinstance(chunk, list):
                all_data.extend(chunk)
            else:
                all_data.append(chunk)
            print(f"ğŸ“¦ å·²åŠ è½½ {f}, å½“å‰æ ·æœ¬æ€»æ•°: {len(all_data)}")
        except Exception as e:
            print(f"âš ï¸ åŠ è½½ {f} å¤±è´¥: {e}")

    print(f"\nâœ… æ•°æ®åŠ è½½å®Œæˆï¼Œæ€»æ ·æœ¬æ•°: {len(all_data)}")
    return all_data


def train_test_split(data_list, train_ratio=0.8):
    """
    ç®€å•éšæœºåˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚
    ï¼ˆä¹Ÿå¯ä»¥æ”¹æˆæŒ‰ data.t_idx / data.stress åšæ›´é«˜çº§çš„åˆ’åˆ†ï¼‰
    """
    indices = list(range(len(data_list)))
    random.shuffle(indices)

    train_size = int(len(indices) * train_ratio)
    train_idx = indices[:train_size]
    test_idx  = indices[train_size:]

    train_data = [data_list[i] for i in train_idx]
    test_data  = [data_list[i] for i in test_idx]

    print(f"ğŸ“Š è®­ç»ƒé›†: {len(train_data)} æ ·æœ¬, æµ‹è¯•é›†: {len(test_data)} æ ·æœ¬")
    return train_data, test_data


# ============ è¯„ä¼°å‡½æ•° ============

def evaluate_model(model, loader, device, return_arrays=False):
    """
    åœ¨ç»™å®š DataLoader ä¸Šè¯„ä¼°ï¼š
    - åªå¯¹ mask=True çš„ sgen èŠ‚ç‚¹è®¡ç®— MAE/RMSE/R2ã€‚
    - å¦‚æœ return_arrays=Trueï¼Œåˆ™é¢å¤–è¿”å› y_true, y_predï¼ˆnumpy æ•°ç»„ï¼‰
    """
    model.eval()
    all_true = []
    all_pred = []

    with torch.no_grad():
        for batch in loader:
            batch = batch.to(device)

            # å‰å‘
            pred = model(batch)   # å½¢çŠ¶ [total_nodes_in_batch, 1] æˆ– [total_nodes]
            if pred.dim() == 1:
                pred = pred.unsqueeze(-1)

            target = batch.y      # [total_nodes, 1]
            mask   = batch.mask   # [total_nodes]

            if mask.sum() == 0:
                continue

            pred_sgen   = pred[mask].view(-1).cpu().numpy()
            target_sgen = target[mask].view(-1).cpu().numpy()

            all_pred.append(pred_sgen)
            all_true.append(target_sgen)

    if len(all_true) == 0:
        print("âš ï¸ æµ‹è¯•é›†ä¸­æ²¡æœ‰æœ‰æ•ˆçš„ sgen èŠ‚ç‚¹ã€‚")
        if return_arrays:
            return None, None, None, None, None
        else:
            return None, None, None

    y_true = np.concatenate(all_true, axis=0)
    y_pred = np.concatenate(all_pred, axis=0)

    mae  = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    r2   = r2_score(y_true, y_pred)

    if return_arrays:
        return mae, rmse, r2, y_true, y_pred
    else:
        return mae, rmse, r2


# ============ ä¸»è®­ç»ƒæµç¨‹ ============

def main():
    print(f"ğŸš€ å¯åŠ¨ç¦»çº¿è®­ç»ƒ GNN")
    print(f"ğŸ“‚ æ•°æ®ç›®å½•: {DATA_DIR}")
    print(f"ğŸ§  è®¾å¤‡: {DEVICE}")

    # 1. åŠ è½½æ•°æ®
    all_data = load_all_chunks(DATA_DIR)

    # ï¼ˆå¯é€‰ï¼‰è¿‡æ»¤ä¸€ä¸‹ Dataï¼Œç¡®ä¿éƒ½æœ‰ y å’Œ mask
    filtered = []
    for d in all_data:
        if hasattr(d, "y") and hasattr(d, "mask"):
            filtered.append(d)
    if len(filtered) < len(all_data):
        print(f"âš ï¸ æœ‰ {len(all_data) - len(filtered)} ä¸ªæ ·æœ¬ç¼ºå°‘ y/maskï¼Œè¢«ä¸¢å¼ƒ")
    all_data = filtered

    # 2. åˆ’åˆ†è®­ç»ƒé›† / æµ‹è¯•é›†
    train_data, test_data = train_test_split(all_data, train_ratio=0.8)

    train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)
    test_loader  = DataLoader(test_data,  batch_size=BATCH_SIZE, shuffle=False)

    # 3. åˆå§‹åŒ–æ¨¡å‹å’Œä¼˜åŒ–å™¨
    #   æ³¨æ„ï¼šnum_node_features=6, num_edge_features=4ï¼Œéœ€è¦å’Œç”Ÿæˆæ•°æ®è„šæœ¬å®Œå…¨ä¸€è‡´
    model = GridGNN(
        num_node_features=6,
        num_edge_features=4,
        hidden_dim=HIDDEN_DIM,
        heads=HEADS
    ).to(DEVICE)

    optimizer = optim.Adam(model.parameters(), lr=LR)

    best_val_mae = float("inf")

    print(f"\n{'Epoch':<6} | {'TrainLoss':<10} | {'ValMAE':<10} | {'BestMAE':<10}")
    print("-" * 60)

    for epoch in range(1, EPOCHS + 1):
        model.train()
        epoch_losses = []

        for batch in train_loader:
            batch = batch.to(DEVICE)

            optimizer.zero_grad()

            pred = model(batch)   # [total_nodes, 1] or [total_nodes]
            if pred.dim() == 1:
                pred = pred.unsqueeze(-1)

            target = batch.y      # [total_nodes, 1]
            mask   = batch.mask   # [total_nodes]

            if mask.sum() == 0:
                continue

            loss = F.smooth_l1_loss(
                pred[mask],
                target[mask],
                beta=0.1
            )
            loss.backward()

            # æ¢¯åº¦è£å‰ªï¼ˆå’Œä½ åœ¨çº¿ç‰ˆä¸€è‡´ï¼‰
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

            optimizer.step()
            epoch_losses.append(loss.item())

        # ä¸€ä¸ª epoch å®Œæˆåçš„å¹³å‡ loss
        if len(epoch_losses) > 0:
            train_loss = float(np.mean(epoch_losses))
        else:
            train_loss = float("nan")

        # æ¯ N ä¸ª epoch åšä¸€æ¬¡éªŒè¯ï¼ˆè¿™é‡Œæ¯ä¸ª epoch éƒ½åšä¹Ÿè¡Œï¼‰
        mae, rmse, r2 = evaluate_model(model, test_loader, DEVICE)
        if mae is not None and mae < best_val_mae:
            best_val_mae = mae
            torch.save(model.state_dict(), SAVE_PATH)

        if mae is not None:
            print(f"{epoch:<6} | {train_loss:<10.6f} | {mae:<10.6f} | {best_val_mae:<10.6f}")
        else:
            print(f"{epoch:<6} | {train_loss:<10.6f} | {'nan':<10} | {best_val_mae:<10.6f}")

    print("\nğŸ‰ è®­ç»ƒç»“æŸï¼")
    print(f"ğŸ’¾ æœ€ä½³æ¨¡å‹å·²ä¿å­˜åˆ°: {SAVE_PATH}")

    # æœ€ç»ˆå†ç”¨æœ€ä½³æ¨¡å‹è¯„ä¼°ä¸€æ¬¡ï¼Œæ‰“å°æœ€ç»ˆæŒ‡æ ‡ + ç”»æ•£ç‚¹å›¾
    try:
        model.load_state_dict(torch.load(SAVE_PATH, map_location=DEVICE))
    except Exception as e:
        print(f"âš ï¸ æ— æ³•é‡æ–°åŠ è½½æœ€ä½³æ¨¡å‹æƒé‡: {e}")

    final_mae, final_rmse, final_r2, y_true, y_pred = evaluate_model(
        model, test_loader, DEVICE, return_arrays=True
    )
    if final_mae is not None:
        print("\nğŸ† æœ€ç»ˆæµ‹è¯•é›†æŒ‡æ ‡:")
        print(f"   MAE  : {final_mae:.6f}")
        print(f"   RMSE : {final_rmse:.6f}")
        print(f"   R2   : {final_r2:.6f}")

        # ========= å®é™…åŠ¨ä½œ vs ç†æƒ³åŠ¨ä½œ æ•£ç‚¹å›¾ =========
        # é»˜è®¤ï¼šy_true = ç†æƒ³åŠ¨ä½œï¼ˆæ ‡ç­¾ï¼‰ï¼Œy_pred = GNN é¢„æµ‹åŠ¨ä½œ
        # å¦‚ç‚¹å¤ªå¤šå¯ä»¥åœ¨è¿™é‡ŒåŠ é‡‡æ ·
        # idx = np.random.choice(len(y_true), size=min(10000, len(y_true)), replace=False)
        # y_true_plot = y_true[idx]
        # y_pred_plot = y_pred[idx]
        # ç°åœ¨å…ˆç”¨å…¨é‡
        y_true_plot = y_true
        y_pred_plot = y_pred

        plt.figure(figsize=(6, 6))

        # ä½¿ç”¨ hexbin æŒ‰é¢‘æ¬¡ç€è‰²ï¼Œbins='log' è®©é¢œè‰²æŒ‰å¯¹æ•°ç¼©æ”¾ï¼Œçœ‹èµ·æ¥æ›´å¹³æ»‘
        hb = plt.hexbin(
            y_true_plot,
            y_pred_plot,
            gridsize=60,  # æ ¼å­æ•°é‡ï¼Œå¯ä»¥è°ƒå¤§/è°ƒå°
            mincnt=1,  # åªæœ‰è‡³å°‘æœ‰ 1 ä¸ªç‚¹çš„æ ¼å­æ‰ç”»
            bins='log'  # é¢œè‰²æŒ‰ log(count) æ˜¾ç¤º
        )
        cb = plt.colorbar(hb)
        cb.set_label("log10(count)")  # é¢œè‰²æ¡æ ‡ç­¾ï¼šç‚¹çš„å¯¹æ•°æ•°é‡

        # ç”»ä¸€æ¡ y = x å‚è€ƒçº¿ï¼ˆå®Œç¾é¢„æµ‹ï¼‰
        min_val = min(np.min(y_true_plot), np.min(y_pred_plot))
        max_val = max(np.max(y_true_plot), np.max(y_pred_plot))
        plt.plot([min_val, max_val], [min_val, max_val], linestyle="--")

        plt.xlabel("Ideal action (OPF target, $\\alpha_{true}$)")
        plt.ylabel("Predicted action (GNN output, $\\alpha_{pred}$)")
        plt.title("Action density plot: predicted vs. ideal (all 132 generators)")
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig("action_density_hexbin.png", dpi=300)
        print("ğŸ“ˆ å¯†åº¦æ•£ç‚¹å›¾å·²ä¿å­˜ä¸º: action_density_hexbin.png")

if __name__ == "__main__":
    main()
